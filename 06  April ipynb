Question 1

Question 1 : What is mathematical formula for a linear SVM ?
Answer :
The mathematical formula for a linear Support Vector Machine (SVM) can be expressed as follows:
Given a training dataset 
 where 
 and 
, the objective of a linear SVM is to find a hyperplane 
 that separates the two classes with maximum margin, where 
 is the weight vector perpendicular to the hyperplane and 
 is the bias term.
The optimization problem can be formulated as:
 

The above optimization problem can be solved using quadratic programming (QP) methods. The dual form of this problem can be derived using the Lagrange duality theory and is given by:
 

Here, 
 are the Lagrange multipliers, and 
 is a hyperparameter that controls the tradeoff between maximizing the margin and minimizing the classification error.
Once we obtain the optimal values of 
, the weight vector 
 and bias term 
 can be computed as follows:

where 
 is any support vector for which 
.

Question 2

Question 2 : What is objective function of a linear SVM ?
Answer :
The objective function of a linear Support Vector Machine (SVM) is to find the optimal hyperplane that separates the two classes in the given dataset with the maximum margin. The margin is defined as the distance between the hyperplane and the closest data point from either of the two classes.
The objective function of a linear SVM can be formulated as a convex optimization problem with the following goal:
Minimize: 
 

Subject to: 
, 

Here, 
 is the weight vector perpendicular to the hyperplane, 
 is the bias term, 
 is the 
th data point, 
 is the corresponding class label, and 
 is the number of data points in the training dataset.
The optimization problem is formulated in such a way that it finds the maximum margin hyperplane that separates the two classes correctly. The optimization problem is solved by finding the Lagrange multipliers 
, which represent the importance of each data point in the optimization problem. The optimal hyperplane is then determined by the support vectors, which are the data points that have non-zero Lagrange multipliers. The support vectors lie on the margin, and the distance between the hyperplane and the support vectors is the margin.
In summary, the objective of a linear SVM is to maximize the margin between the hyperplane and the support vectors, subject to the constraint that all data points are correctly classified.

Question 3

Question 3 : What is kernel trick in SVM ?
Answer :
The kernel trick is a technique used in Support Vector Machines (SVMs) to implicitly map input data into a higher-dimensional space, without actually computing the mapping explicitly. This allows the SVM to efficiently handle non-linearly separable data by finding a non-linear decision boundary in the higher-dimensional space.
In a linear SVM, the decision boundary is a hyperplane in the input space. However, for non-linearly separable data, a linear decision boundary may not be able to separate the classes well. In such cases, the kernel trick can be used to map the input data into a higher-dimensional space where the classes are more easily separable.
The kernel function is used to compute the dot product of two feature vectors in the higher-dimensional space, without actually computing the mapping explicitly. Different types of kernel functions can be used, such as polynomial, radial basis function (RBF), and sigmoid kernels.
The most commonly used kernel function is the RBF kernel, which is defined as:
Minimize: 
 

Subject to: 
, 


where 
 and 
 are the input feature vectors, and 
 is a hyperparameter that controls the smoothness of the decision boundary.

The RBF kernel has a high value when the feature vectors are close to each other and decreases exponentially as the distance between them increases.
By using the kernel trick, the SVM can find a non-linear decision boundary in the higher-dimensional space, without explicitly computing the mapping. This allows the SVM to handle complex, non-linearly separable data efficiently.
In summary, the kernel trick in SVM is a technique to implicitly map input data into a higher-dimensional space using a kernel function, which allows the SVM to handle non-linearly separable data and find a non-linear decision boundary in the higher-dimensional space.

Question 4

Question 4 : What is role of support vectors in SVM Explain with example
Answer :
In Support Vector Machines (SVMs), the support vectors are the data points that lie closest to the decision boundary. These points are called support vectors because they "support" the decision boundary by defining its position and orientation in the feature space.
The role of the support vectors in SVM is crucial because the decision boundary is determined by these points, and all other data points have no effect on it. The support vectors are the ones that define the margin, which is the distance between the decision boundary and the closest data points of each class. The SVM algorithm aims to maximize this margin, while still correctly classifying all data points.
To illustrate the role of support vectors in SVM, consider the following example. Suppose we have a binary classification problem where the goal is to classify points as either red or blue. The data points are shown in the figure below:
Support Vectors

A linear SVM is used to find a decision boundary that separates the two classes.
In this example, the two data points closest to the decision boundary are the support vectors. These are the points that "support" the decision boundary and determine its position and orientation. All other data points have no effect on the decision boundary.
The margin of the SVM is the distance between the decision boundary and the closest data points of each class. In this example, the margin is shown as the two dashed lines.
The SVM algorithm aims to maximize this margin, while still correctly classifying all data points. The presence of the support vectors ensures that the decision boundary is robust to small changes in the data, since it is determined only by the points closest to it.
In summary, the support vectors are the data points that lie closest to the decision boundary in SVM. They define the position and orientation of the decision boundary and determine the margin of the SVM. The presence of the support vectors ensures that the decision boundary is robust to small changes in the data.

Question 5

Question 5 : Illustrate with examples and graphs Hyperplane , Marginal Plane, Soft Margin, Hard Margin in SVM.
Answer :
1. Hyperplane
In SVM, a hyperplane is a decision boundary that separates the data into different classes. In a binary classification problem, a hyperplane is a line that separates the data points of one class from the data points of the other class. In a multiclass classification problem, a hyperplane is a plane or a higher-dimensional hyperplane that separates the data points of each class from the data points of the other classes.

Hyperplane

In this example, the blue and red dots represent two different classes of data points. The hyperplane is a line that separates the blue dots from the red dots. Any new data point that falls on the blue side of the line will be classified as belonging to the blue class, while any new data point that falls on the red side of the line will be classified as belonging to the red class.

2. Marginal Plane
The marginal plane is the hyperplane that is equidistant from the support vectors on each side. The distance between the marginal plane and the closest support vector is called the margin, and it is maximized in SVM.

3. Hard Margin
In SVM, the hard margin method is used when the data is linearly separable, meaning that a hyperplane can be found that perfectly separates the data into two classes. The hard margin method tries to find the hyperplane that maximizes the margin while ensuring that all data points are correctly classified.

4. Soft Margin
In SVM, the soft margin method is used when the data is not linearly separable, meaning that a hyperplane cannot be found that perfectly separates the data into two classes. The soft margin method allows for some misclassification of the data points in order to find a hyperplane that separates the data as well as possible.

Soft Margin vs Hard margin


Question 6

Question 6 : SVM Implementation through IRIS dataset
Load the IRIS dataset from scikit-learn library and split it into training and testing set

Train a linear SVM classifier on training set and predict labels for testing set

Compute accuracy of model on testing set

Plot decision boundaries of the trained model using two of the features

Try different values of regularisation parameter C and see how its performance affects the performance of model

Answer :
Load the IRIS dataset from scikit-learn library and split it into training and testing set
# Load IRIS dataset from sklearn
from sklearn.datasets import load_iris
dataset = load_iris()
print(dataset.DESCR)
.. _iris_dataset:

Iris plants dataset
--------------------

**Data Set Characteristics:**

    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica
                
    :Summary Statistics:

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
    ============== ==== ==== ======= ===== ====================

    :Missing Attribute Values: None
    :Class Distribution: 33.3% for each of 3 classes.
    :Creator: R.A. Fisher
    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
    :Date: July, 1988

The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
from Fisher's paper. Note that it's the same as in R, but not as in the UCI
Machine Learning Repository, which has two wrong data points.

This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher's paper is a classic in the field and
is referenced frequently to this day.  (See Duda & Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.

.. topic:: References

   - Fisher, R.A. "The use of multiple measurements in taxonomic problems"
     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to
     Mathematical Statistics" (John Wiley, NY, 1950).
   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.
     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.
   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System
     Structure and Classification Rule for Recognition in Partially Exposed
     Environments".  IEEE Transactions on Pattern Analysis and Machine
     Intelligence, Vol. PAMI-2, No. 1, 67-71.
   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions
     on Information Theory, May 1972, 431-433.
   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II
     conceptual clustering system finds 3 classes in the data.
   - Many, many more ...
dataset.keys()
dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])
import pandas as pd
X =pd.DataFrame(dataset.data, columns=dataset.feature_names)
X.head()
sepal length (cm)	sepal width (cm)	petal length (cm)	petal width (cm)
0	5.1	3.5	1.4	0.2
1	4.9	3.0	1.4	0.2
2	4.7	3.2	1.3	0.2
3	4.6	3.1	1.5	0.2
4	5.0	3.6	1.4	0.2
dataset.target_names
array(['setosa', 'versicolor', 'virginica'], dtype='<U10')
Y = pd.DataFrame(dataset.target, columns=['class'])
Y.head()
class
0	0
1	0
2	0
3	0
4	0
# Train test split
from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(X,Y,test_size=0.33,random_state=42)
xtrain.shape
(100, 4)
xtest.shape
(50, 4)
Train a linear SVM classifier on training set and predict labels for testing set
# Train a linear SVM classifier on the training set
from sklearn.svm import SVC
svm = SVC(kernel='linear', C=1.0)
svm.fit(xtrain, ytrain.values.flatten())
SVC(kernel='linear')
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.
# Predict the labels for the testing set
y_pred = svm.predict(xtest)

# Compute the accuracy of the model on the testing set
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(ytest, y_pred)
print("Testing Accuracy:", accuracy)
Testing Accuracy: 1.0
from sklearn.metrics import confusion_matrix
import seaborn as sns
cf = confusion_matrix(ytest,y_pred)
sns.heatmap(cf,annot=True)
<Axes: >

Plot Decision Boundaries
# Plot the decision boundaries of the trained model using two of the features
import seaborn as sns
import matplotlib.pyplot as plt
df_train = pd.concat([xtrain,ytrain],axis=1)
sns.scatterplot(data = df_train, x = 'sepal length (cm)',y='sepal width (cm)',hue='class')
plt.title('Sepal Length vs Sepal width hueplot')
plt.show()

# Plot the decision boundaries of the trained model using two of the features
import seaborn as sns
import matplotlib.pyplot as plt
df_train = pd.concat([xtrain,ytrain],axis=1)
sns.scatterplot(data = df_train, x = 'petal length (cm)',y='petal width (cm)',hue='class')
plt.title('Petal length vs Petal width hueplot')
plt.show()

# Pairplot
sns.pairplot(df_train,hue='class')
plt.show()

Try different values of regularisation parameter C and see how its performance affects the performance of model
import numpy as np
C = np.linspace(0.01,10,20)
C
array([ 0.01      ,  0.53578947,  1.06157895,  1.58736842,  2.11315789,
        2.63894737,  3.16473684,  3.69052632,  4.21631579,  4.74210526,
        5.26789474,  5.79368421,  6.31947368,  6.84526316,  7.37105263,
        7.89684211,  8.42263158,  8.94842105,  9.47421053, 10.        ])
from sklearn.metrics import classification_report
acc = []
for i in C:
    model = SVC(kernel='linear', C=i)
    model.fit(xtrain,ytrain.values.flatten())
    ytest_pred = model.predict(xtest)
    acc.append(accuracy_score(ytest,ytest_pred))
    print(f'C Value : {i}\n')
    print(classification_report(ytest,ytest_pred))
    print('\n======================================================================\n')
C Value : 0.01

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 0.5357894736842106

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 1.0615789473684212

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 1.5873684210526318

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 2.113157894736842

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 2.6389473684210527

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 3.1647368421052633

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 3.690526315789474

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 4.2163157894736845

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 4.742105263157895

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      0.93      0.97        15
           2       0.94      1.00      0.97        16

    accuracy                           0.98        50
   macro avg       0.98      0.98      0.98        50
weighted avg       0.98      0.98      0.98        50


======================================================================

C Value : 5.267894736842106

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      0.93      0.97        15
           2       0.94      1.00      0.97        16

    accuracy                           0.98        50
   macro avg       0.98      0.98      0.98        50
weighted avg       0.98      0.98      0.98        50


======================================================================

C Value : 5.793684210526316

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      0.93      0.97        15
           2       0.94      1.00      0.97        16

    accuracy                           0.98        50
   macro avg       0.98      0.98      0.98        50
weighted avg       0.98      0.98      0.98        50


======================================================================

C Value : 6.319473684210527

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      0.93      0.97        15
           2       0.94      1.00      0.97        16

    accuracy                           0.98        50
   macro avg       0.98      0.98      0.98        50
weighted avg       0.98      0.98      0.98        50


======================================================================

C Value : 6.845263157894737

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      0.93      0.97        15
           2       0.94      1.00      0.97        16

    accuracy                           0.98        50
   macro avg       0.98      0.98      0.98        50
weighted avg       0.98      0.98      0.98        50


======================================================================

C Value : 7.371052631578948

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      0.93      0.97        15
           2       0.94      1.00      0.97        16

    accuracy                           0.98        50
   macro avg       0.98      0.98      0.98        50
weighted avg       0.98      0.98      0.98        50


======================================================================

C Value : 7.8968421052631586

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      0.93      0.97        15
           2       0.94      1.00      0.97        16

    accuracy                           0.98        50
   macro avg       0.98      0.98      0.98        50
weighted avg       0.98      0.98      0.98        50


======================================================================

C Value : 8.42263157894737

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 8.948421052631579

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 9.47421052631579

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

C Value : 10.0

              precision    recall  f1-score   support

           0       1.00      1.00      1.00        19
           1       1.00      1.00      1.00        15
           2       1.00      1.00      1.00        16

    accuracy                           1.00        50
   macro avg       1.00      1.00      1.00        50
weighted avg       1.00      1.00      1.00        50


======================================================================

import matplotlib.pyplot as plt
plt.plot(C, acc)
plt.xlabel('C value')
plt.ylabel('Testing Accuracy')
plt.title('Accuracies for different values of C')
plt.show()
